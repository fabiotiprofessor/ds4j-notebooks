{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Extracting topics from text documents\n",
                "\n",
                "Sometimes you have a nice big set of documents, and all you wish for is to know what's hiding inside. But without reading them, of course! Two approaches to try to lazily get some information from your texts are **topic modeling** and **clustering**."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<p class=\"reading-options\">\n  <a class=\"btn\" href=\"/text-analysis/topic-modeling-and-clustering\">\n    <i class=\"fa fa-sm fa-book\"></i>\n    Read online\n  </a>\n  <a class=\"btn\" href=\"/text-analysis/notebooks/Topic modeling and clustering.ipynb\">\n    <i class=\"fa fa-sm fa-download\"></i>\n    Download notebook\n  </a>\n  <a class=\"btn\" href=\"#\">\n    <i class=\"fa fa-sm fa-laptop\"></i>\n    Interactive version\n  </a>\n</p>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "I'm going to tell you a big secret: **computers are really really really bad at reading documents and figuring out what they're about.** Text is for _people_ to read, people with a collective knowledge of The World At Large and a history of reading things and all kinds of other tricky secret little things we don't think about that help us understand what a piece of text means.\n",
                "\n",
                "When dealing with understanding content, computers are good for _very specific situations_ to do _very specific things_. Or alternatively, to do a not-that-great job when you aren't going to be terribly picky about the results.\n",
                "\n",
                "Do I sound a little biased? Oh, but aren't we all. It isn't going to stop us from talking about it, though!\n",
                "\n",
                "Before we start, **let's make some assumptions:**\n",
                "\n",
                "* When you're dealing with documents, each document is (typically) about something.\n",
                "* You know each document is about by looking at the words in the document.\n",
                "* Documents with similar words are probably about similar things. \n",
                "\n",
                "We have two major options available to us: **topic modeling** and **clustering**. There's a lot of NLP nuance going on between the two, but we're going to keep it simple:\n",
                "\n",
                "**Topic modeling** is if each document can be about **multiple topics**. There might be 100 different topics, and a document might be 30% about one topic, 20% about another, and then 50% spread out between the others.\n",
                "\n",
                "**Clustering** is if each document should only fit into **one topic**. It's an all-or-nothing approach.\n",
                "\n",
                "The most important part of _all of this_ is the fact that **the computer figures out these topics by itself**. You don't tell it what to do! If you're teaching the algorithm what different specific topics look like, that's **classification.** In this case we're just saying \"hey computer, please figure this out!\"\n",
                "\n",
                "Let's get started."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preparing our datasets\n",
                "\n",
                "### Recipes\n",
                "\n",
                "We're going to start with analyzing **about 36,000 recipes**. Food is interesting because you can split it so many ways: by courses, or by baked goods vs meat vs vegetables vs others, by national cuisine..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>cuisine</th>\n",
                            "      <th>id</th>\n",
                            "      <th>ingredient_list</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>greek</td>\n",
                            "      <td>10259</td>\n",
                            "      <td>romaine lettuce, black olives, grape tomatoes, garlic, pepper, purple onion, seasoning, garbanzo beans, feta cheese crumbles</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>southern_us</td>\n",
                            "      <td>25693</td>\n",
                            "      <td>plain flour, ground pepper, salt, tomatoes, ground black pepper, thyme, eggs, green tomatoes, yellow corn meal, milk, vegetable oil</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>filipino</td>\n",
                            "      <td>20130</td>\n",
                            "      <td>eggs, pepper, salt, mayonaise, cooking oil, green chilies, grilled chicken breasts, garlic powder, yellow onion, soy sauce, butter, chicken livers</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>indian</td>\n",
                            "      <td>22213</td>\n",
                            "      <td>water, vegetable oil, wheat, salt</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>indian</td>\n",
                            "      <td>13162</td>\n",
                            "      <td>black pepper, shallots, cornflour, cayenne pepper, onions, garlic paste, milk, butter, salt, lemon juice, water, chili powder, passata, oil, ground cumin, boneless chicken skinless thigh, garam ma...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "       cuisine     id  \\\n",
                            "0        greek  10259   \n",
                            "1  southern_us  25693   \n",
                            "2     filipino  20130   \n",
                            "3       indian  22213   \n",
                            "4       indian  13162   \n",
                            "\n",
                            "                                                                                                                                                                                           ingredient_list  \n",
                            "0                                                                             romaine lettuce, black olives, grape tomatoes, garlic, pepper, purple onion, seasoning, garbanzo beans, feta cheese crumbles  \n",
                            "1                                                                      plain flour, ground pepper, salt, tomatoes, ground black pepper, thyme, eggs, green tomatoes, yellow corn meal, milk, vegetable oil  \n",
                            "2                                                       eggs, pepper, salt, mayonaise, cooking oil, green chilies, grilled chicken breasts, garlic powder, yellow onion, soy sauce, butter, chicken livers  \n",
                            "3                                                                                                                                                                        water, vegetable oil, wheat, salt  \n",
                            "4  black pepper, shallots, cornflour, cayenne pepper, onions, garlic paste, milk, butter, salt, lemon juice, water, chili powder, passata, oil, ground cumin, boneless chicken skinless thigh, garam ma...  "
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "pd.set_option(\"display.max_colwidth\", 200)\n",
                "\n",
                "recipes = pd.read_csv(\"data/recipes.csv\")\n",
                "recipes.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In order to analyze the text, we'll need to count the words in each recipe. To do that we're going to use a **stemmed TF-IDF vectorizer** from scikit-learn.\n",
                "\n",
                "* **Stemming** will allow us to combine words like `tomato` and `tomatoes`\n",
                "* Using **TF-IDF** will allow us to devalue common ingredients like salt and water\n",
                "\n",
                "I'm using the code from [the reference section](https://investigate.ai/reference/vectorizing/#stem-and-vectorize), just adjusted from a `CountVectorizer` to a `TfidfVectorizer`, and set it so ingredients have to appear in at least **fifty recipes**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>activ</th>\n",
                            "      <th>adobo</th>\n",
                            "      <th>agav</th>\n",
                            "      <th>alfredo</th>\n",
                            "      <th>all</th>\n",
                            "      <th>allspic</th>\n",
                            "      <th>almond</th>\n",
                            "      <th>amchur</th>\n",
                            "      <th>anaheim</th>\n",
                            "      <th>ancho</th>\n",
                            "      <th>...</th>\n",
                            "      <th>wrapper</th>\n",
                            "      <th>yam</th>\n",
                            "      <th>yeast</th>\n",
                            "      <th>yellow</th>\n",
                            "      <th>yoghurt</th>\n",
                            "      <th>yogurt</th>\n",
                            "      <th>yolk</th>\n",
                            "      <th>yukon</th>\n",
                            "      <th>zest</th>\n",
                            "      <th>zucchini</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.278745</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.276000</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.210575</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>5 rows \u00d7 752 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   activ  adobo  agav  alfredo  all  allspic  almond  amchur  anaheim  ancho  \\\n",
                            "0    0.0    0.0   0.0      0.0  0.0      0.0     0.0     0.0      0.0    0.0   \n",
                            "1    0.0    0.0   0.0      0.0  0.0      0.0     0.0     0.0      0.0    0.0   \n",
                            "2    0.0    0.0   0.0      0.0  0.0      0.0     0.0     0.0      0.0    0.0   \n",
                            "3    0.0    0.0   0.0      0.0  0.0      0.0     0.0     0.0      0.0    0.0   \n",
                            "4    0.0    0.0   0.0      0.0  0.0      0.0     0.0     0.0      0.0    0.0   \n",
                            "\n",
                            "   ...  wrapper  yam  yeast    yellow  yoghurt    yogurt  yolk  yukon  zest  \\\n",
                            "0  ...      0.0  0.0    0.0  0.000000      0.0  0.000000   0.0    0.0   0.0   \n",
                            "1  ...      0.0  0.0    0.0  0.278745      0.0  0.000000   0.0    0.0   0.0   \n",
                            "2  ...      0.0  0.0    0.0  0.276000      0.0  0.000000   0.0    0.0   0.0   \n",
                            "3  ...      0.0  0.0    0.0  0.000000      0.0  0.000000   0.0    0.0   0.0   \n",
                            "4  ...      0.0  0.0    0.0  0.000000      0.0  0.210575   0.0    0.0   0.0   \n",
                            "\n",
                            "   zucchini  \n",
                            "0       0.0  \n",
                            "1       0.0  \n",
                            "2       0.0  \n",
                            "3       0.0  \n",
                            "4       0.0  \n",
                            "\n",
                            "[5 rows x 752 columns]"
                        ]
                    },
                    "execution_count": 53,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "import Stemmer\n",
                "\n",
                "# English stemmer from pyStemmer\n",
                "stemmer = Stemmer.Stemmer('en')\n",
                "\n",
                "analyzer = TfidfVectorizer().build_analyzer()\n",
                "\n",
                "# Override TfidfVectorizer\n",
                "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
                "    def build_analyzer(self):\n",
                "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
                "        return lambda doc: stemmer.stemWords(analyzer(doc))\n",
                "\n",
                "vectorizer = StemmedTfidfVectorizer(min_df=50)\n",
                "matrix = vectorizer.fit_transform(recipes.ingredient_list)\n",
                "\n",
                "words_df = pd.DataFrame(matrix.toarray(),\n",
                "                        columns=vectorizer.get_feature_names())\n",
                "words_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Looks like we have 752 ingredients! Yes, there are some numbers in there and probably other things we aren't interested in, but let's stick with it for now."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Topic modeling\n",
                "\n",
                "There are multiple techniques for topic modeling, but in the end they do the same thing: **you get a list of topics, and a list of words associated with each topic.**\n",
                "\n",
                "Let's tell it to break them down into **five topics.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
                            "    n_components=5, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
                            "    verbose=0)"
                        ]
                    },
                    "execution_count": 54,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sklearn.decomposition import NMF\n",
                "\n",
                "model = NMF(n_components=5)\n",
                "model.fit(matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Why five topics? **Because we have to tell it _something_.** Our job is to decide the number of topics, and it's the computer's job to find the topics. We'll talk about how to pick the \"right\" number later, but for now: it's magic.\n",
                "\n",
                "Fitting the model allowed it to \"learn\" what the ingredients are and how they're organized, we just need to find out what's inside. Let's ask for the **top ten terms in each group.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Topic #0: oliv pepper fresh oil dri garlic salt parsley red tomato\n",
                        "Topic #1: flour egg sugar purpos all butter bake milk larg powder\n",
                        "Topic #2: sauc soy sesam rice oil ginger sugar chicken vinegar garlic\n",
                        "Topic #3: ground chili cilantro cumin powder lime onion pepper chop fresh\n",
                        "Topic #4: chees shred cream parmesan cheddar grate tortilla mozzarella sour chicken\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "n_words = 10\n",
                "feature_names = vectorizer.get_feature_names()\n",
                "\n",
                "for topic_idx, topic in enumerate(model.components_):\n",
                "    message = \"Topic #%d: \" % topic_idx\n",
                "    message += \" \".join([feature_names[i]\n",
                "                         for i in topic.argsort()[:-n_words - 1:-1]])\n",
                "    print(message)\n",
                "print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Those actually seem like _pretty good topics_. Italian-ish, then baking, then Chinese, maybe Latin American or Indian food, and then dairy. What if we did it with **fifteen topics** instead?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Topic #0: pepper bell red green onion celeri flake black tomato crush\n",
                        "Topic #1: flour purpos all bake powder butter soda buttermilk salt egg\n",
                        "Topic #2: sauc soy sesam oil ginger rice sugar garlic scallion starch\n",
                        "Topic #3: tortilla cream shred chees sour cheddar salsa corn bean jack\n",
                        "Topic #4: chees parmesan grate mozzarella pasta ricotta basil italian fresh spinach\n",
                        "Topic #5: lime cilantro fresh chop juic jalapeno chile avocado chili fish\n",
                        "Topic #6: chicken breast boneless skinless broth halv sodium low fat thigh\n",
                        "Topic #7: ground black pepper cumin cinnamon salt beef cayenn kosher paprika\n",
                        "Topic #8: chili seed powder cumin coriand masala garam curri ginger coconut\n",
                        "Topic #9: sugar egg vanilla milk extract larg cream butter yolk unsalt\n",
                        "Topic #10: oliv extra virgin oil clove garlic fresh salt tomato parsley\n",
                        "Topic #11: white wine vinegar rice shallot red salt grain mustard sugar\n",
                        "Topic #12: dri oregano tomato thyme parsley garlic bay basil leaf onion\n",
                        "Topic #13: lemon juic fresh orang zest parsley grate mint peel yogurt\n",
                        "Topic #14: water yeast warm sugar salt cold flour activ boil ice\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "model = NMF(n_components=15)\n",
                "model.fit(matrix)\n",
                "\n",
                "n_words = 10\n",
                "feature_names = vectorizer.get_feature_names()\n",
                "\n",
                "for topic_idx, topic in enumerate(model.components_):\n",
                "    message = \"Topic #%d: \" % topic_idx\n",
                "    message += \" \".join([feature_names[i]\n",
                "                         for i in topic.argsort()[:-n_words - 1:-1]])\n",
                "    print(message)\n",
                "print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is where we start to see **the big difference between categories and topics**. The grouping with five groups seemed very much like cuisines - Italian, Chinese, etc. But now that we're breaking it down further, the groups have changed a bit.\n",
                "\n",
                "They're now **more like classes of ingredients.** Baking gets a category - `chicken breast boneless skinless` and so do generic European ingredients - `oliv extra virgin oil clove garlic fresh salt`. The algorithm got a little confused about black pepper vs. hot pepper flakes vs green/yellow bell peppers when it created `pepper bell red green onion celeri flake black`, but we understand what it's going for.\n",
                "\n",
                "Remember, the important thing about topic modeling is that every row in our dataset is a **combinations of topics**. It might be a little bit about one thing, a little bit less about another, etc etc. Let's take a look at how that works."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>topic_0</th>\n",
                            "      <th>topic_1</th>\n",
                            "      <th>topic_2</th>\n",
                            "      <th>topic_3</th>\n",
                            "      <th>topic_4</th>\n",
                            "      <th>topic_5</th>\n",
                            "      <th>topic_6</th>\n",
                            "      <th>topic_7</th>\n",
                            "      <th>topic_8</th>\n",
                            "      <th>topic_9</th>\n",
                            "      <th>topic_10</th>\n",
                            "      <th>topic_11</th>\n",
                            "      <th>topic_12</th>\n",
                            "      <th>topic_13</th>\n",
                            "      <th>topic_14</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1.438302</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>2.665898</td>\n",
                            "      <td>1.615467</td>\n",
                            "      <td>0.22474</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.507691</td>\n",
                            "      <td>0.00000</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>2.561061</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.480989</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>2.447042</td>\n",
                            "      <td>3.041425</td>\n",
                            "      <td>0.176429</td>\n",
                            "      <td>1.408264</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.00000</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>5.715500</td>\n",
                            "      <td>0.49568</td>\n",
                            "      <td>1.583069</td>\n",
                            "      <td>0.548611</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>1.620243</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.243763</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "    topic_0   topic_1   topic_2   topic_3   topic_4  topic_5  topic_6  \\\n",
                            "0  1.438302  0.000000  0.000000  2.665898  1.615467  0.22474      0.0   \n",
                            "1  2.447042  3.041425  0.176429  1.408264  0.000000  0.00000      0.0   \n",
                            "\n",
                            "    topic_7  topic_8   topic_9  topic_10  topic_11  topic_12  topic_13  \\\n",
                            "0  0.507691  0.00000  0.000000  2.561061       0.0  0.480989       0.0   \n",
                            "1  5.715500  0.49568  1.583069  0.548611       0.0  1.620243       0.0   \n",
                            "\n",
                            "   topic_14  \n",
                            "0  0.000000  \n",
                            "1  0.243763  "
                        ]
                    },
                    "execution_count": 59,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Get a list of topics - yes, they're named poorly\n",
                "topic_list = [f\"topic_{i}\" for i in range(model.n_components_)]\n",
                "\n",
                "# Convert our counts into numbers\n",
                "percentages = model.transform(matrix)\n",
                "\n",
                "# Set it up as a dataframe\n",
                "topics = pd.DataFrame(percentages, columns=topic_list)\n",
                "topics.head(2) * 100"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Our first recipe is primary `topic_3` with a rating of 2.44, but it's also a bit topic 0 and topic 8 with scores of 1.5 and 1.36.\n",
                "\n",
                "Our second recipe is a bit bolder - it scores a whopping 5.7 in `topic_7`, with 0, 8 and 14 coming up in the 2.5-3 range.\n",
                "\n",
                "Let's combing this topics dataframe with our **original dataframe** to see what is going on here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "merged = recipes.merge(topics, right_index=True, left_index=True)\n",
                "merged.head(2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we can do things like...\n",
                "\n",
                "* Uncover possible topics discussed in the dataset\n",
                "* See how many documents cover each topic\n",
                "* Find the top documents in each topic\n",
                "\n",
                "There's a lot lot lot more to say on topic modeling - other techniques, other critiques, as well as fun visualizations - so be sure to check out the follow-up after you read the comparison with clustering down below!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clustering\n",
                "\n",
                "Clustering's major difference is that **each category is kept completely separate.**\n",
                "\n",
                "Let's do the same thing with clustering that with did with topic modeling, starting with breaking things into **five categories.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 7min 37s, sys: 2.42 s, total: 7min 40s\n",
                        "Wall time: 7min 54s\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
                            "       n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
                            "       random_state=None, tol=0.0001, verbose=0)"
                        ]
                    },
                    "execution_count": 57,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "from sklearn.cluster import KMeans\n",
                "\n",
                "km = KMeans(n_clusters=5)\n",
                "km.fit(matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It's a fair bit slower, but that's also because we picked the absolute fastest version of topic modeling that exists.\n",
                "\n",
                "Let's see what the top 10 words are for each of the clusters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top terms per cluster:\n",
                        "Cluster 0: ground chili cilantro cumin lime\n",
                        "Cluster 1: sugar flour egg butter purpos\n",
                        "Cluster 2: pepper oliv fresh oil salt\n",
                        "Cluster 3: sauc soy sesam oil rice\n",
                        "Cluster 4: chees shred cream parmesan cheddar\n"
                    ]
                }
            ],
            "source": [
                "print(\"Top terms per cluster:\")\n",
                "\n",
                "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
                "terms = vectorizer.get_feature_names()\n",
                "n_words = 10\n",
                "for i in range(km.n_clusters):\n",
                "    top_ten_words = [terms[ind] for ind in order_centroids[i, :n_words]]\n",
                "    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Nothing too surprising there! Pretty much the same thing we got with topic modeling. Now let's break it into **fifteen groups**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top terms per cluster:\n",
                        "Cluster 0: lemon juic fresh oliv pepper\n",
                        "Cluster 1: lime cilantro fresh juic chili\n",
                        "Cluster 2: sauc soy sesam oil ginger\n",
                        "Cluster 3: flour purpos all bake egg\n",
                        "Cluster 4: pepper bell green onion red\n",
                        "Cluster 5: salt water sugar butter pepper\n",
                        "Cluster 6: chees tortilla shred cheddar cream\n",
                        "Cluster 7: dri oliv fresh pepper tomato\n",
                        "Cluster 8: vanilla sugar extract egg cream\n",
                        "Cluster 9: chees parmesan grate mozzarella pepper\n",
                        "Cluster 10: fish sauc lime fresh thai\n",
                        "Cluster 11: seed masala chili cumin coriand\n",
                        "Cluster 12: chicken boneless breast skinless halv\n",
                        "Cluster 13: virgin extra oliv oil fresh\n",
                        "Cluster 14: ground pepper cumin salt black\n",
                        "CPU times: user 11min 53s, sys: 17.5 s, total: 12min 10s\n",
                        "Wall time: 11min 1s\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "km = KMeans(n_clusters=15)\n",
                "km.fit(matrix)\n",
                "\n",
                "print(\"Top terms per cluster:\")\n",
                "n_words = 10\n",
                "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
                "terms = vectorizer.get_feature_names()\n",
                "for i in range(km.n_clusters):\n",
                "    top_ten_words = [terms[ind] for ind in order_centroids[i, :n_words]]\n",
                "    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top terms per cluster:\n",
                        "Cluster 0: vanilla extract sugar egg butter\n",
                        "Cluster 1: pepper bell green onion red\n",
                        "Cluster 2: flour all purpos egg salt\n",
                        "Cluster 3: chees parmesan grate mozzarella pepper\n",
                        "Cluster 4: salt pepper onion oil water\n",
                        "Cluster 5: ground pepper cumin salt powder\n",
                        "Cluster 6: chicken boneless breast skinless halv\n",
                        "Cluster 7: bake flour powder purpos all\n",
                        "Cluster 8: seed chili masala cumin coriand\n",
                        "Cluster 9: virgin extra oliv oil fresh\n",
                        "Cluster 10: sauc soy oil ginger starch\n",
                        "Cluster 11: sesam soy seed sauc oil\n",
                        "Cluster 12: coconut milk curri past lime\n",
                        "Cluster 13: dri oliv tomato fresh wine\n",
                        "Cluster 14: lemon juic fresh oliv pepper\n",
                        "Cluster 15: chees tortilla shred cheddar cream\n",
                        "Cluster 16: sugar cream orang milk egg\n",
                        "Cluster 17: fish sauc lime fresh peanut\n",
                        "Cluster 18: lime cilantro fresh chili juic\n",
                        "Cluster 19: flat leaf parsley oliv pepper\n",
                        "CPU times: user 11min 2s, sys: 15 s, total: 11min 17s\n",
                        "Wall time: 10min 27s\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "km = KMeans(n_clusters=20)\n",
                "km.fit(matrix)\n",
                "\n",
                "print(\"Top terms per cluster:\")\n",
                "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
                "terms = vectorizer.get_feature_names()\n",
                "for i in range(km.n_clusters):\n",
                "    top_ten_words = [terms[ind] for ind in order_centroids[i, :10]]\n",
                "    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "\n",
                "km = KMeans(n_clusters=30)\n",
                "km.fit(matrix)\n",
                "\n",
                "print(\"Top terms per cluster:\")\n",
                "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
                "terms = vectorizer.get_feature_names()\n",
                "for i in range(km.n_clusters):\n",
                "    top_ten_words = [terms[ind] for ind in order_centroids[i, :5]]\n",
                "    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": true,
            "toc_window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}